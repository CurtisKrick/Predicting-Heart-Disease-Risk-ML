{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14571,"status":"ok","timestamp":1733548238422,"user":{"displayName":"Ben Zimmerman","userId":"13805190401710635928"},"user_tz":360},"id":"YqHnDQnIzem3","outputId":"54eb9fb8-0f02-440a-e76a-0a7880066dd9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"6BBMoFE4zmM9","executionInfo":{"status":"ok","timestamp":1733771360414,"user_tz":360,"elapsed":10155,"user":{"displayName":"Milo Etz","userId":"13208177152289294831"}}},"outputs":[],"source":["from pyspark.sql import SparkSession\n","import pyspark.sql.functions as sql_f\n","import pandas as pd\n","\n","spark = SparkSession\\\n","    .builder\\\n","    .master(\"local[*]\")\\\n","    .appName(\"Project_Learning\")\\\n","    .getOrCreate()\n","\n","sc = spark.sparkContext"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_M0vRk66znur"},"outputs":[],"source":["df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/content/drive/MyDrive/CSC590_Project/data/combined_data.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KxeydWLP0jhO","executionInfo":{"status":"ok","timestamp":1733548271651,"user_tz":360,"elapsed":2281,"user":{"displayName":"Ben Zimmerman","userId":"13805190401710635928"}},"outputId":"91fdaaf7-37bd-431a-f098-f9f6150d4143"},"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- DifficultyWalking: string (nullable = true)\n"," |-- AlcoholDrinking: string (nullable = true)\n"," |-- PhysicalHealthDays: integer (nullable = true)\n"," |-- Smoking: string (nullable = true)\n"," |-- Diabetic: string (nullable = true)\n"," |-- AgeCategory: string (nullable = true)\n"," |-- PhysicalActivity: string (nullable = true)\n"," |-- HadStroke: string (nullable = true)\n"," |-- Sex: string (nullable = true)\n"," |-- MentalHealthDays: integer (nullable = true)\n"," |-- HadAsthma: string (nullable = true)\n"," |-- HadKidneyDisease: string (nullable = true)\n"," |-- Race: string (nullable = true)\n"," |-- GeneralHealth: string (nullable = true)\n"," |-- BMI: double (nullable = true)\n"," |-- HadHeartAttack: string (nullable = true)\n"," |-- SleepHours: integer (nullable = true)\n"," |-- HadSkinCancer: string (nullable = true)\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["565817"]},"metadata":{},"execution_count":4}],"source":["df.printSchema()\n","df.count()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q43X-UNZy4Jp"},"outputs":[],"source":["from pyspark.ml import Pipeline\n","from pyspark.ml.classification import RandomForestClassifier\n","from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n","import math\n","\n","# List of categorical columns to be indexed\n","categorical_columns = [\n","    'DifficultyWalking', 'AlcoholDrinking', 'Smoking', 'Diabetic',\n","    'AgeCategory', 'PhysicalActivity', 'HadStroke', 'Sex',\n","    'HadAsthma', 'HadKidneyDisease', 'Race', 'GeneralHealth',\n","    'HadSkinCancer'\n","]\n","\n","# List of categorical columns to be one-hot encoded\n","one_hot_columns = [\n","    'Diabetic', 'Race'\n","]\n","\n","# List of numerical columns\n","numerical_columns = [\n","    'PhysicalHealthDays', 'MentalHealthDays', 'BMI', 'SleepHours'\n","]\n","\n","# Explicitly remap the target variable to numeric\n","df = df.withColumn(\"HadHeartAttack_Numeric\",\n","    sql_f.when(sql_f.col(\"HadHeartAttack\") == \"Yes\", 1.0)\n","    .otherwise(0.0)\n",")\n","\n","# Create StringIndexers for categorical columns\n","indexers = [\n","    StringIndexer(inputCol=column, outputCol=column+\"_indexed\", handleInvalid=\"keep\")\n","    for column in categorical_columns\n","]\n","\n","# OneHotEncode only Diabetic and Race\n","ohe = [\n","    OneHotEncoder(inputCol=\"Diabetic_indexed\", outputCol=\"Diabetic_ohe\"),\n","    OneHotEncoder(inputCol=\"Race_indexed\", outputCol=\"Race_ohe\")\n","]\n","\n","# Combine all features\n","feature_columns = [col+\"_indexed\" for col in categorical_columns if col not in [\"Diabetic\", \"Race\"]] \\\n","                  + [\"Diabetic_ohe\", \"Race_ohe\"] \\\n","                  + numerical_columns\n","\n","# Assemble features into a single vector column\n","assembler = VectorAssembler(\n","    inputCols=feature_columns,\n","    outputCol=\"features\",\n","    handleInvalid=\"skip\"\n",")\n","\n","# address the class imbalance\n","minority_count = df.filter(sql_f.col(\"HadHeartAttack_Numeric\") == 1).count()\n","majority_count = df.filter(sql_f.col(\"HadHeartAttack_Numeric\") == 0).count()\n","total_count = df.count()\n","\n","# ---------------------------------------\n","# Different weighting strategies\n","# ---------------------------------------\n","# 1. No weighting (all 1.0)\n","df = df.withColumn(\"weight_no\", sql_f.lit(1.0))\n","\n","# 2. Ratio weighting (as in your original approach)\n","ratio = minority_count / float(majority_count)\n","df = df.withColumn(\"weight_ratio\",\n","    sql_f.when(sql_f.col(\"HadHeartAttack_Numeric\") == 0, ratio).otherwise(1.0)\n",")\n","\n","# 3. Square root ratio\n","sqrt_ratio = math.sqrt(ratio)\n","\n","df = df.withColumn(\"weight_sqrt_ratio\",\n","    sql_f.when(sql_f.col(\"HadHeartAttack_Numeric\") == 0, sqrt_ratio).otherwise(1.0)\n",")\n","\n","# 4. Hybrid weighting (ratio and sqrt_ratio)\n","adjusted_ratio = (ratio + sqrt_ratio) / 2.0\n","df = df.withColumn(\"weight_adjusted_ratio\",\n","    sql_f.when(sql_f.col(\"HadHeartAttack_Numeric\") == 0, adjusted_ratio).otherwise(1.0)\n",")\n","\n","# 5. Inverse frequency weighting (weights are inverse to class frequency)\n","#    For binary classification, you can define something like:\n","#    weight = total_count / (count_of_class * number_of_classes)\n","#    This gives minority class higher weight.\n","minority_weight = (total_count / (minority_count * 2.0))\n","majority_weight = (total_count / (majority_count * 2.0))\n","df = df.withColumn(\"weight_invfreq\",\n","    sql_f.when(sql_f.col(\"HadHeartAttack_Numeric\") == 1, minority_weight).otherwise(majority_weight)\n",")\n","\n","# 6. Logarithmic ratio\n","log_ratio = math.log1p(ratio)  # log(1 + ratio)\n","df = df.withColumn(\"weight_log_ratio\",\n","    sql_f.when(sql_f.col(\"HadHeartAttack_Numeric\") == 0, log_ratio).otherwise(1.0)\n",")\n","\n","# Split the data\n","train_data, test_data = df.randomSplit([0.7, 0.3], seed=123)\n","\n","rf = RandomForestClassifier(\n","    labelCol=\"HadHeartAttack_Numeric\",\n","    featuresCol=\"features\",\n","    seed=123\n",")\n","\n","pipeline = Pipeline(stages=indexers + ohe + [assembler, rf])\n","\n","# Create a parameter grid that tries out different weighting columns.\n","# Note: weightCol should be a column name existing in the DataFrame.\n","paramGrid = (ParamGridBuilder()\n","    .addGrid(rf.numTrees, [100])\n","    .addGrid(rf.maxDepth, [10])\n","    #.addGrid(rf.weightCol, [\"weight_no\", \"weight_ratio\", \"weight_sqrt_ratio\", \"weight_adjusted_ratio\", \"weight_invfreq\", \"weight_log_ratio\"])\n","    .addGrid(rf.weightCol, [\"weight_ratio\", \"weight_sqrt_ratio\", \"weight_adjusted_ratio\", \"weight_invfreq\", \"weight_log_ratio\"])\n","    .build()\n",")\n","\n","evaluator = BinaryClassificationEvaluator(\n","    labelCol=\"HadHeartAttack_Numeric\",\n","    rawPredictionCol=\"rawPrediction\",\n","    metricName=\"areaUnderPR\"\n",")\n","\n","crossval = CrossValidator(\n","    estimator=pipeline,\n","    estimatorParamMaps=paramGrid,\n","    evaluator=evaluator,\n","    numFolds=2,\n","    seed=123\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vtyVqFp40u9S"},"outputs":[],"source":["# Fit the cross-validator\n","cv_model = crossval.fit(train_data)\n","\n","# Get the best model\n","best_model = cv_model.bestModel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ej27iX243wrW"},"outputs":[],"source":["# Make predictions using the best model\n","predictions = best_model.transform(test_data)"]},{"cell_type":"code","source":["# Retrieve the average cross-validation metrics for each parameter combination\n","metrics = cv_model.avgMetrics  # One metric per ParamMap in paramGrid\n","\n","# Pair each metric with the corresponding parameter set from paramGrid\n","param_map_and_metrics = list(zip(paramGrid, metrics))\n","\n","# Create a dictionary to store the best results per weight strategy\n","best_per_weight = {}\n","\n","for param_map, metric in param_map_and_metrics:\n","    # Directly access parameter values from the ParamMap\n","    numTrees = param_map[rf.numTrees]\n","    maxDepth = param_map[rf.maxDepth]\n","    weightCol = param_map[rf.weightCol]\n","\n","    # If we haven't seen this weightCol yet, or if this result is better than a previous one, update it\n","    if weightCol not in best_per_weight or metric > best_per_weight[weightCol][\"metric\"]:\n","        best_per_weight[weightCol] = {\n","            \"numTrees\": numTrees,\n","            \"maxDepth\": maxDepth,\n","            \"metric\": metric\n","        }\n","\n","# Sort the best strategies by their metric in descending order\n","best_strategies_sorted = sorted(best_per_weight.items(), key=lambda x: x[1][\"metric\"], reverse=True)\n","\n","print(\"Best Result from Each Weight Strategy:\")\n","for weightCol, info in best_strategies_sorted:\n","    print(f\"\\nWeight Column: {weightCol}\")\n","    print(f\"  numTrees: {info['numTrees']}\")\n","    print(f\"  maxDepth: {info['maxDepth']}\")\n","    print(f\"  Average CV Metric (Area Under PR): {info['metric']}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"niY15km4w7cz","executionInfo":{"status":"ok","timestamp":1733554439049,"user_tz":360,"elapsed":3,"user":{"displayName":"Ben Zimmerman","userId":"13805190401710635928"}},"outputId":"ca781cf1-53ad-48e9-f45b-cba26d8672a0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Best Result from Each Weight Strategy:\n","\n","Weight Column: weight_sqrt_ratio\n","  numTrees: 100\n","  maxDepth: 10\n","  Average CV Metric (Area Under PR): 0.29479956375322686\n","\n","Weight Column: weight_adjusted_ratio\n","  numTrees: 100\n","  maxDepth: 10\n","  Average CV Metric (Area Under PR): 0.2926725744884703\n","\n","Weight Column: weight_ratio\n","  numTrees: 100\n","  maxDepth: 10\n","  Average CV Metric (Area Under PR): 0.28584454082850047\n","\n","Weight Column: weight_log_ratio\n","  numTrees: 100\n","  maxDepth: 10\n","  Average CV Metric (Area Under PR): 0.2856961035175649\n","\n","Weight Column: weight_invfreq\n","  numTrees: 100\n","  maxDepth: 10\n","  Average CV Metric (Area Under PR): 0.28563687112281244\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mik1AcWV9H0-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733554513766,"user_tz":360,"elapsed":74719,"user":{"displayName":"Ben Zimmerman","userId":"13805190401710635928"}},"outputId":"371af9d6-05da-45fc-d0f6-619169649a4b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Best Hyperparameters:\n","maxDepth: 10\n","numTrees: 100\n","weightCol: weight_sqrt_ratio\n","\n","Area Under PR Curve (AUC): 0.2860954534340103\n","\n","Confusion Matrix:\n","[[152020.   5551.]\n"," [  8788.   3132.]]\n","\n","Class 0.0:\n","  Precision: 0.9453509775633053\n","  Recall: 0.9647714363683673\n","  F1 Score: 0.9549624818219794\n","\n","Class 1.0:\n","  Precision: 0.36070482552113325\n","  Recall: 0.262751677852349\n","  F1 Score: 0.30403339319516576\n","\n","Feature Importances:\n","AgeCategory_indexed: 0.23444493852467702\n","HadStroke_indexed: 0.14397770783202324\n","DifficultyWalking_indexed: 0.1401660608789722\n","GeneralHealth_indexed: 0.08733611953656205\n","Race_ohe: 0.07165750568222788\n","Diabetic_ohe: 0.06451648208327365\n","Sex_indexed: 0.050396613322974834\n","Smoking_indexed: 0.03671769641306137\n","HadKidneyDisease_indexed: 0.029503834830845403\n","AlcoholDrinking_indexed: 0.019516572940138716\n","HadSkinCancer_indexed: 0.010125354781691962\n","PhysicalActivity_indexed: 0.006330038941189653\n","BMI: 0.005676626307444707\n","HadAsthma_indexed: 0.002652238578379918\n","SleepHours: 0.002288468989539327\n","PhysicalHealthDays: 0.0014291942100807975\n","MentalHealthDays: 0.0010489275951778257\n"]}],"source":["from pyspark.mllib.evaluation import MulticlassMetrics\n","\n","# Extract and print the best parameters\n","best_rf_model = best_model.stages[-1]\n","best_params = best_rf_model.extractParamMap()\n","\n","print(\"Best Hyperparameters:\")\n","for param, value in best_params.items():\n","    if \"weightCol\" in param.name or \"numTrees\" in param.name or \"maxDepth\" in param.name:\n","        print(f\"{param.name}: {value}\")\n","\n","# Evaluate the model\n","auc_pr = evaluator.evaluate(predictions)\n","print(f\"\\nArea Under PR Curve (AUC): {auc_pr}\")\n","\n","# Display the confusion matrix\n","predictions_and_labels = predictions.select(['prediction', 'HadHeartAttack_Numeric']).rdd.map(lambda x: (float(x[0]), float(x[1])))\n","metrics = MulticlassMetrics(predictions_and_labels)\n","print(\"\\nConfusion Matrix:\")\n","print(metrics.confusionMatrix().toArray())\n","\n","for label in [0.0, 1.0]:\n","    print(f\"\\nClass {label}:\")\n","    print(f\"  Precision: {metrics.precision(label)}\")\n","    print(f\"  Recall: {metrics.recall(label)}\")\n","    print(f\"  F1 Score: {metrics.fMeasure(label)}\")\n","\n","# Feature Importance\n","rf_model = best_model.stages[-1]\n","feature_importances = list(zip(feature_columns, rf_model.featureImportances))\n","feature_importances.sort(key=lambda x: x[1], reverse=True)\n","\n","print(\"\\nFeature Importances:\")\n","for feature, importance in feature_importances:\n","    print(f\"{feature}: {importance}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"epdW-KKcu82W"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}